var documenterSearchIndex = {"docs":
[{"location":"functions/#Functions","page":"Functions","title":"Functions","text":"","category":"section"},{"location":"functions/#Data-structures","page":"Functions","title":"Data structures","text":"","category":"section"},{"location":"functions/","page":"Functions","title":"Functions","text":"Modules = [GigaSOM]\nPages = [\"structs.jl\"]","category":"page"},{"location":"functions/#GigaSOM.LoadedDataInfo","page":"Functions","title":"GigaSOM.LoadedDataInfo","text":"LoadedDataInfo\n\nThe basic structure for working with loaded data, distributed amongst workers. In completeness, it represents a dataset as such:\n\nval is the \"value name\" under which the data are saved in processes. E.g. val=:foo means that there is a variable foo on each process holding a part of the matrix.\nworkers is a list of workers (in correct order!) that hold the data (similar to DArray.pids)\n\n\n\n\n\n","category":"type"},{"location":"functions/#GigaSOM.Som","page":"Functions","title":"GigaSOM.Som","text":"Som\n\nStructure to hold all data of a trained SOM.\n\nFields:\n\ncodes::Array{Float64,2}: 2D-array of codebook vectors. One vector per row\nxdim::Int: number of neurons in x-direction\nydim::Int: number of neurons in y-direction\nnumCodes::Int: total number of neurons\ngrid::Array{Float64,2}: 2D-array of coordinates of neurons on the map         (2 columns (x,y)] for rectangular and hexagonal maps          3 columns (x,y,z) for spherical maps)\n\n\n\n\n\n","category":"type"},{"location":"functions/#Data-loading-and-preparation","page":"Functions","title":"Data loading and preparation","text":"","category":"section"},{"location":"functions/","page":"Functions","title":"Functions","text":"Modules = [GigaSOM]\nPages = [\"input.jl\", \"process.jl\", \"splitting.jl\", \"dataops.jl\"]","category":"page"},{"location":"functions/#GigaSOM.distributeFCSFileVector","page":"Functions","title":"GigaSOM.distributeFCSFileVector","text":"distributeFCSFileVector(name::Symbol, fns::Vector{String}, pids=workers())::LoadedDataInfo\n\nDistribute a vector of integers among the workers that describes which file from fns the cell comes from. Useful for producing per-file statistics. The vector is saved on workers specified by pids as a distributed variable name.\n\n\n\n\n\n","category":"function"},{"location":"functions/#GigaSOM.distributeFileVector","page":"Functions","title":"GigaSOM.distributeFileVector","text":"distributeFileVector(name::Symbol, sizes::Vector{Int}, slices::Vector{Tuple{Int,Int,Int,Int}}, pids=workers())::LoadedDataInfo\n\nGeneralized version of distributeFCSFileVector that produces the integer vector from any sizes and slices.\n\n\n\n\n\n","category":"function"},{"location":"functions/#GigaSOM.getCSVSize-Tuple{String}","page":"Functions","title":"GigaSOM.getCSVSize","text":"function getCSVSize(fn::String; args...)::Tuple{Int,Int}\n\nRead the dimensions (number of rows and columns, respectively) from a CSV file fn. args are passed to function CSV.file.\n\nExample\n\ngetCSVSize(\"test.csv\", header=false)\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.getFCSSize-Tuple{Any, Any}","page":"Functions","title":"GigaSOM.getFCSSize","text":"getFCSSize(offsets, params)::Tuple{Int,Int}\n\nConvert the offsets and keywords from an FCS file to cell and parameter count, respectively.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.loadCSV-Tuple{String}","page":"Functions","title":"GigaSOM.loadCSV","text":"function loadCSV(fn::String; args...)::Matrix{Float64}\n\nCSV equivalent of loadFCS. The metadata (header, column names) are not extracted. args are passed to CSV.read.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.loadCSVSet","page":"Functions","title":"GigaSOM.loadCSVSet","text":"function loadCSVSet(\n    name::Symbol,\n    fns::Vector{String},\n    pids = workers();\n    postLoad = (d, i) -> d,\n    csvargs...,\n)::LoadedDataInfo\n\nCSV equivalent of loadFCSSet. csvargs are passed as keyword arguments to CSV-loading functions.\n\n\n\n\n\n","category":"function"},{"location":"functions/#GigaSOM.loadCSVSizes-Tuple{Vector{String}}","page":"Functions","title":"GigaSOM.loadCSVSizes","text":"function loadCSVSizes(fns::Vector{String}; args...)::Vector{Int}\n\nDetermine number of rows in a list of CSV files (passed as fns). Equivalent to loadFCSSizes.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.loadFCS-Tuple{String}","page":"Functions","title":"GigaSOM.loadFCS","text":"loadFCS(fn::String; applyCompensation::Bool=true)::Tuple{Dict{String,String}, Matrix{Float64}}\n\nRead a FCS file. Return a tuple that contains in order:\n\ndictionary of the keywords contained in the file\nraw column names\nprettified and annotated column names\nraw data matrix\n\nIf applyCompensation is set, the function parses and retrieves a spillover matrix (if any valid keyword in the FCS is found that would contain it) and applies it to compensate the data.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.loadFCSHeader-Tuple{String}","page":"Functions","title":"GigaSOM.loadFCSHeader","text":"loadFCSHeader(fn::String)::Tuple{Vector{Int}, Dict{String,String}}\n\nEfficiently extract data offsets and keyword dictionary from an FCS file.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.loadFCSSet","page":"Functions","title":"GigaSOM.loadFCSSet","text":"loadFCSSet(name::Symbol, fns::Vector{String}, pids=workers(); applyCompensation=true, postLoad=(d,i)->d)::LoadedDataInfo\n\nThis runs the FCS loading machinery in a distributed way, so that the files fns (with full path) are sliced into equal parts and saved as a distributed variable name on workers specified by pids.\n\napplyCompensation is passed to loadFCS function.\n\nSee slicesof for description of the slicing.\n\npostLoad is applied to the loaded FCS file data (and the index) – use this function to e.g. filter out certain columns right on loading, using selectFCSColumns.\n\nThe loaded dataset can be manipulated by the distributed functions, e.g.\n\ndselect for removing columns\ndscale for normalization\ndtransform_asinh (and others) for transformation\netc.\n\n\n\n\n\n","category":"function"},{"location":"functions/#GigaSOM.loadFCSSizes-Tuple{Vector{String}}","page":"Functions","title":"GigaSOM.loadFCSSizes","text":"loadFCSSizes(fns::Vector{String})\n\nLoad cell counts in many FCS files at once. Useful as input for slicesof.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.selectFCSColumns-Tuple{Vector{String}}","page":"Functions","title":"GigaSOM.selectFCSColumns","text":"selectFCSColumns(selectColnames::Vector{String})\n\nReturn a function useful with loadFCSSet, which loads only the specified (prettified) column names from the FCS files. Use getMetaData, getMarkerNames and cleanNames! to retrieve the usable column names for a FCS.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.cleanNames!-Tuple{Vector{String}}","page":"Functions","title":"GigaSOM.cleanNames!","text":"cleanNames!(mydata::Vector{String})\n\nReplaces problematic characters in column names, avoids duplicate names, and prefixes an '_' if the name starts with a number.\n\nArguments:\n\nmydata: vector of names (gets modified)\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.compensate!-Tuple{Matrix{Float64}, Matrix{Float64}, Vector{Int64}}","page":"Functions","title":"GigaSOM.compensate!","text":"compensate!(data::Matrix{Float64}, spillover::Matrix{Float64}, cols::Vector{Int})\n\nApply a compensation matrix in spillover (the individual columns of which describe, in order, the spillover of cols in data) to the matrix data in-place.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.getMarkerNames-Tuple{DataFrames.DataFrame}","page":"Functions","title":"GigaSOM.getMarkerNames","text":"getMarkerNames(meta::DataFrame)::Tuple{Vector{String}, Vector{String}}\n\nExtract suitable raw names (useful for selecting columns) and pretty readable names (useful for humans) from FCS file metadata.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.getMetaData-Tuple{Dict{String, String}}","page":"Functions","title":"GigaSOM.getMetaData","text":"getMetaData(f)\n\nCollect the meta data information in a more user friendly format.\n\nArguments:\n\nf: input structure with .params and .data fields\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.getSpillover-Tuple{Dict{String, String}}","page":"Functions","title":"GigaSOM.getSpillover","text":"getSpillover(params::Dict{String, String})::Union{Tuple{Vector{String},Matrix{Float64}}, Nothing}\n\nGet a spillover matrix from FCS params. Returns a pair with description of columns to be applied, and with the actual spillover matrix. Returns nothing in case spillover is not present.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.parseSpillover-Tuple{String}","page":"Functions","title":"GigaSOM.parseSpillover","text":"parseSpillover(str::String)::Union{Tuple{Vector{String},Matrix{Float64}}, Nothing}\n\nParses the spillover matrix from the string from FCS parameter value.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.collectSlice-Tuple{Any, NTuple{4, Int64}}","page":"Functions","title":"GigaSOM.collectSlice","text":"collectSlice(loadVec, (startFile, startOff, finalFile, finalOff)::Tuple{Int,Int,Int,Int})::Vector\n\nAlternative of vcollectSlice for 1D vectors.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.slicesof-Tuple{Vector{Int64}, Int64}","page":"Functions","title":"GigaSOM.slicesof","text":"slicesof(lengths::Vector{Int}, slices::Int)::Vector{Tuple{Int,Int,Int,Int}}\n\nGiven a list of lengths of input arrays, compute a slicing into a specified amount of equally-sized slices.\n\nThe output is a vector of 4-tuples where each specifies how to create one slice. The i-th tuple field contains, in order:\n\nthe index of input array at which the i-th slice begins\nfirst element of the i-th slice in that input array\nthe index of input array with the last element of the i-th slice\nthe index of the last element of the i-th slice in that array\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.vcollectSlice-Tuple{Any, NTuple{4, Int64}}","page":"Functions","title":"GigaSOM.vcollectSlice","text":"vcollectSlice(loadMtx, (startFile, startOff, finalFile, finalOff)::Tuple{Int,Int,Int,Int})::Matrix\n\nGiven a method to obtain matrix content (loadMtx), reconstruct a slice from the information generated by slicesof.\n\nThis function is specialized for reconstructing matrices and arrays, where the \"element counts\" split by slicesof are in fact matrix rows. The function is therefore named vcollect (the slicing and concatenation is vertical).\n\nThe actual data content and loading method is abstracted out – function loadMtx gets the index of the input part that it is required to fetch (e.g. index of one FCS file), and is expected to return that input part as a whole matrix. vcollectSlice correctly calls this function as required and extracts relevant portions of the matrices, so that at the end the whole slice can be pasted together.\n\nExample:\n\n# get a list of files\nfilenames=[\"a.fcs\", \"b.fcs\"]\n# get descriptions of 5 equally sized parts of the data\nslices = slicesof(loadFCSSizes(filenames), 5)\n\n# reconstruct first 3 columns of the first slice\nmySlice = vcollectSlice(\n    i -> last(loadFCS(slices[i]))[:,1:3],\n    slices[1])\n# (note: function loadFCS returns 4 items, the matrix is the last one)\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.catmapbuckets-Tuple{Any, Array, Int64, Vector{Int64}}","page":"Functions","title":"GigaSOM.catmapbuckets","text":"catmapbuckets(fn, a::Array, nbuckets::Int, buckets::Vector{Int}; bucketdim::Int=1)\n\nSame as mapbuckets, except concatenates the bucketing results in the bucketing dimension, thus creating a slightly neater matrix. slicedims is therefore fixed to bucketdim.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.collect_extrema-Tuple{Any, Any}","page":"Functions","title":"GigaSOM.collect_extrema","text":"collect_extrema(ex1, ex2)\n\nHelper for collecting the minimums and maximums of the data. ex1, ex2 are arrays of pairs (min,max), this function combines the arrays element-wise and finds combined minima and maxima.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.combine_stats-Tuple{Any, Any}","page":"Functions","title":"GigaSOM.combine_stats","text":"combine_stats((s1, sqs1, n1), (s2, sqs2, n2))\n\nHelper for dstat-style functions that just adds up elements in triplets of vectors.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.dapply_cols-Tuple{LoadedDataInfo, Any, Vector{Int64}}","page":"Functions","title":"GigaSOM.dapply_cols","text":"dapply_cols(dInfo::LoadedDataInfo, fn, columns::Vector{Int})\n\nApply a function fn over columns of a distributed dataset.\n\nfn gets 2 parameters:\n\na data vector for (the whole column saved at one worker)\nindex of the column in the columns array (i.e. a number from 1:length(columns))\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.dapply_rows-Tuple{LoadedDataInfo, Any}","page":"Functions","title":"GigaSOM.dapply_rows","text":"dapply_rows(dInfo::LoadedDataInfo, fn)\n\nApply a function fn over rows of a distributed dataset.\n\nfn gets a single vector parameter for each row to transform.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.dcopy-Tuple{LoadedDataInfo, Symbol}","page":"Functions","title":"GigaSOM.dcopy","text":"dcopy(dInfo::LoadedDataInfo, newName::Symbol)\n\nClone the dataset and store it under a new distributed name newName.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.dcount-Tuple{Int64, LoadedDataInfo}","page":"Functions","title":"GigaSOM.dcount","text":"dcount(ncats::Int, dInfo::LoadedDataInfo)::Vector{Int}\n\nCount the numbers of integer vector values stored in dInfo; assuming the values are in range 1–ncats.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.dcount_buckets-Tuple{Int64, LoadedDataInfo, Int64, LoadedDataInfo}","page":"Functions","title":"GigaSOM.dcount_buckets","text":"dcount_buckets(ncats::Int, dInfo::LoadedDataInfo, nbuckets::Int, buckets::LoadedDataInfo)::Matrix{Int}\n\nSame as dcount, but counts the items in dInfo bucketed by buckets to produce a matrix of counts, with ncats rows and nbuckets columns.\n\nUseful with distributeFCSFileVector to determine cluster distribution within files.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.dmedian-Tuple{LoadedDataInfo, Vector{Int64}}","page":"Functions","title":"GigaSOM.dmedian","text":"dmedian(dInfo::LoadedDataInfo, columns::Vector{Int})\n\nCompute a median in a distributed fashion, avoiding data transfer and memory capacity that is required to compute the median in the classical way by sorting. All data must be finite and defined. If the median is just between 2 values, the lower one is chosen.\n\nThe algorithm is approximative, searching for a good median by halving interval and counting how many values are below the threshold. iters can be increased to improve precision, each value adds roughly 1 bit to the precision. The default value is 20, which corresponds to precision 10e-6 times the data range.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.dmedian_buckets-Tuple{LoadedDataInfo, Int64, LoadedDataInfo, Vector{Int64}}","page":"Functions","title":"GigaSOM.dmedian_buckets","text":"dmedian_buckets(dInfo::LoadedDataInfo, nbuckets::Int, buckets::LoadedDataInfo, columns::Vector{Int}; iters=20)\n\nA version of dmedian that works with the bucketing information (i.e. clusters) from nbuckets and buckets.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.dscale-Tuple{LoadedDataInfo, Vector{Int64}}","page":"Functions","title":"GigaSOM.dscale","text":"dscale(dInfo::LoadedDataInfo, columns::Vector{Int})\n\nScale the columns in the dataset to have mean 0 and sdev 1.\n\nPrevents creation of NaNs by avoiding division by zero sdevs.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.dselect","page":"Functions","title":"GigaSOM.dselect","text":"function dselect(dInfo::LoadedDataInfo,\n    currentColnames::Vector{String}, selectColnames::Vector{String};\n    tgt=dInfo.val)::LoadedDataInfo\n\nConvenience overload of dselect that works with column names.\n\n\n\n\n\n","category":"function"},{"location":"functions/#GigaSOM.dselect-2","page":"Functions","title":"GigaSOM.dselect","text":"dselect(dInfo::LoadedDataInfo, columns::Vector{Int}; tgt=dInfo.val)\n\nReduce dataset to selected columns, optionally save it under a different name.\n\n\n\n\n\n","category":"function"},{"location":"functions/#GigaSOM.dstat-Tuple{LoadedDataInfo, Vector{Int64}}","page":"Functions","title":"GigaSOM.dstat","text":"dstat(dInfo::LoadedDataInfo, columns::Vector{Int})::Tuple{Vector{Float64}, Vector{Float64}}\n\nCompute mean and standard deviation of the columns in dataset. Returns a tuple with a vector of means in columns, and a vector of corresponding sdevs.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.dstat_buckets-Tuple{LoadedDataInfo, Int64, LoadedDataInfo, Vector{Int64}}","page":"Functions","title":"GigaSOM.dstat_buckets","text":"dstat_buckets(dInfo::LoadedDataInfo, nbuckets::Int, buckets::LoadedDataInfo, columns::Vector{Int})::Tuple{Matrix{Float64}, Matrix{Float64}}\n\nA version of dstat that works with bucketing information (e.g. clusters); returns a tuple of matrices.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.dtransform_asinh","page":"Functions","title":"GigaSOM.dtransform_asinh","text":"dtransform_asinh(dInfo::LoadedDataInfo, columns::Vector{Int}, cofactor=5)\n\nTransform columns of the dataset by asinh transformation with cofactor.\n\n\n\n\n\n","category":"function"},{"location":"functions/#GigaSOM.mapbuckets-Tuple{Any, Array, Int64, Vector{Int64}}","page":"Functions","title":"GigaSOM.mapbuckets","text":"mapbuckets(fn, a::Array, nbuckets::Int, buckets::Vector{Int}; bucketdim::Int=1, slicedims=bucketdim)\n\nApply the function fn over array a so that it processes the data by buckets defined by buckets (that contains integers in range 1:nbuckets).\n\nThe buckets are sliced out in dimension specified by bucketdim.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.update_extrema-NTuple{4, Any}","page":"Functions","title":"GigaSOM.update_extrema","text":"update_extrema(counts, target, lim, mid)\n\nHelper for distributed median computation – returns updated extrema in lims depending on whether the item count in counts of values less than mids is less or higher than targets.\n\n\n\n\n\n","category":"method"},{"location":"functions/#SOM-training","page":"Functions","title":"SOM training","text":"","category":"section"},{"location":"functions/","page":"Functions","title":"Functions","text":"Modules = [GigaSOM]\nPages = [\"core.jl\", \"trainutils.jl\"]","category":"page"},{"location":"functions/#GigaSOM.distributedEpoch-Tuple{LoadedDataInfo, Matrix{Float64}, Any}","page":"Functions","title":"GigaSOM.distributedEpoch","text":"distributedEpoch(dInfo::LoadedDataInfo, codes::Matrix{Float64}, tree)\n\nExecute the doEpoch in parallel on workers described by dInfo and collect the results. Returns pair of numerator and denominator matrices.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.doEpoch-Tuple{Matrix{Float64}, Matrix{Float64}, Any}","page":"Functions","title":"GigaSOM.doEpoch","text":"doEpoch(x::Array{Float64, 2}, codes::Array{Float64, 2}, tree)\n\nvectors and the adjustment in radius after each epoch.\n\nArguments:\n\nx: training Data\ncodes: Codebook\ntree: knn-compatible tree built upon the codes\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.initGigaSOM","page":"Functions","title":"GigaSOM.initGigaSOM","text":"function initGigaSOM(ncol::Int64,\n                     means::Vector{Float64}, sdevs::Vector{Float64},\n                     xdim::Int64, ydim::Int64 = xdim;\n                     seed = rand(Int), rng = StableRNG(seed))\n\nGenerate a stable random initial SOM with the random distribution that matches the parameters.\n\nArguments:\n\nncol: number of desired data columns\nmeans, sdevs: vectors that describe the data distribution, both of size ncol\nxdim, ydim: Size of the SOM\nseed: a seed (defaults to random seed from the current default random generator\nrng: a random number generator to be used (defaults to a StableRNG initialized with the seed)\n\nReturns: a new Som structure\n\n\n\n\n\n","category":"function"},{"location":"functions/#GigaSOM.initGigaSOM-Tuple{LoadedDataInfo, Vararg{Any}}","page":"Functions","title":"GigaSOM.initGigaSOM","text":"function initGigaSOM(data::LoadedDataInfo,\n                     xdim::Int64, ydim::Int64 = xdim;\n                     seed=rand(Int), rng=StableRNG(seed))\n\ninitGigaSOM overload for working with distributed-style LoadedDataInfo data. The rest of the arguments is passed to the data-independent initGigaSOM.\n\nArguments:\n\ndata: a LoadedDataInfo object with the distributed dataset matrix\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.initGigaSOM-Tuple{Union{DataFrames.DataFrame, Matrix}, Vararg{Any}}","page":"Functions","title":"GigaSOM.initGigaSOM","text":"initGigaSOM(data, args...)\n\nInitializes a SOM by random selection from the training data. A generic overload that works for matrices and DataFrames that can be coerced to Matrix{Float64}. Other arguments are passed to the data-independent initGigaSOM.\n\nArguments:\n\ndata: matrix of data for running the initialization\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.mapToGigaSOM-Tuple{Som, Any}","page":"Functions","title":"GigaSOM.mapToGigaSOM","text":"mapToGigaSOM(som::Som, data;\n             knnTreeFun = BruteTree,\n             metric = Euclidean())\n\nOverload of mapToGigaSOM for simple DataFrames and matrices. This slices the data using DistributedArrays, sends them the workers, and runs normal mapToGigaSOM. Data is undistributed after the computation.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.mapToGigaSOM-Tuple{Som, LoadedDataInfo}","page":"Functions","title":"GigaSOM.mapToGigaSOM","text":"mapToGigaSOM(som::Som, dInfo::LoadedDataInfo;\n    knnTreeFun = BruteTree, metric = Euclidean(),\n    output::Symbol=tmpSym(dInfo)::LoadedDataInfo\n\nCompute the index of the BMU for each row of the input data.\n\nArguments\n\nsom: a trained SOM\ndInfo: LoadedDataInfo that describes the loaded and distributed data\nknnTreeFun: Constructor of the KNN-tree (e.g. from NearestNeighbors package)\nmetric: Passed as metric argument to the KNN-tree constructor\noutput: Symbol to save the result, defaults to tmpSym(dInfo)\n\nData must have the same number of dimensions as the training dataset and will be normalised with the same parameters.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.scaleEpochTime-Tuple{Int64, Int64}","page":"Functions","title":"GigaSOM.scaleEpochTime","text":"scaleEpochTime(iteration::Int64, epochs::Int64)\n\nConvert iteration ID and epoch number to relative time in training.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.trainGigaSOM-Tuple{Som, Any}","page":"Functions","title":"GigaSOM.trainGigaSOM","text":"trainGigaSOM(som::Som, train;\n             kwargs...)\n\nOverload of trainGigaSOM for simple DataFrames and matrices. This slices the data, distributes them to the workers, and runs normal trainGigaSOM. Data is undistributed after the computation.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.trainGigaSOM-Tuple{Som, LoadedDataInfo}","page":"Functions","title":"GigaSOM.trainGigaSOM","text":"trainGigaSOM(\n    som::Som,\n    dInfo::LoadedDataInfo;\n    kernelFun::Function = gaussianKernel,\n    metric = Euclidean(),\n    somDistFun = distMatrix(Chebyshev()),\n    knnTreeFun = BruteTree,\n    rStart = 0.0,\n    rFinal = 0.1,\n    radiusFun = expRadius(-5.0),\n    epochs = 20,\n    eachEpoch = (e, r, som) -> nothing,\n)\n\nArguments:\n\nsom: object of type Som with an initialised som\ndInfo: LoadedDataInfo object that describes a loaded dataset\nkernelFun::function: optional distance kernel; one of (bubbleKernel, gaussianKernel)           default is gaussianKernel\nmetric: Passed as metric argument to the KNN-tree constructor\nsomDistFun: Function for computing the distances in the SOM map\nknnTreeFun: Constructor of the KNN-tree (e.g. from NearestNeighbors package)\nrStart: optional training radius. If zero (default), it is computed from the SOM grid size.\nrFinal: target radius at the last epoch, defaults to 0.1\nradiusFun: Function that generates radius decay, e.g. linearRadius or expRadius(10.0)\nepochs: number of SOM training iterations (default 10)\neachEpoch: a function to call back after each epoch, accepting arguments (epochNumber, radius, som). For simplicity, this gets additionally called once before the first epoch, with epochNumber set to zero.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.bubbleKernel-Tuple{Any, Float64}","page":"Functions","title":"GigaSOM.bubbleKernel","text":"bubbleKernel(x, r::Float64)\n\nReturn a \"bubble\" (spherical) distribution kernel.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.distMatrix","page":"Functions","title":"GigaSOM.distMatrix","text":"distMatrix(metric=Chebyshev())\n\nReturn a function that uses the metric (compatible with metrics from package Distances) calculates distance matrixes from normal row-wise data matrices, using the metric.\n\nUse as a parameter of trainGigaSOM.\n\n\n\n\n\n","category":"function"},{"location":"functions/#GigaSOM.expRadius","page":"Functions","title":"GigaSOM.expRadius","text":"expRadius(steepness::Float64)\n\nReturn a function to be used as a radiusFun of trainGigaSOM, which causes exponencial decay with the selected steepness.\n\nUse: trainGigaSOM(..., radiusFun = expRadius(0.5))\n\nArguments\n\nsteepness: Steepness of exponential descent. Good values range from -100.0 (almost linear) to 100.0 (really quick decay).\n\n\n\n\n\n","category":"function"},{"location":"functions/#GigaSOM.gaussianKernel-Tuple{Any, Float64}","page":"Functions","title":"GigaSOM.gaussianKernel","text":"gaussianKernel(x, r::Float64)\n\nReturn the value of normal distribution PDF (σ=r, μ=0) at x\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.gridRectangular-Tuple{Any, Any}","page":"Functions","title":"GigaSOM.gridRectangular","text":"gridRectangular(xdim, ydim)\n\nCreate coordinates of all neurons on a rectangular SOM.\n\nThe return-value is an array of size (Number-of-neurons, 2) with x- and y- coordinates of the neurons in the first and second column respectively. The distance between neighbours is 1.0. The point of origin is bottom-left. The first neuron sits at (0,0).\n\nArguments\n\nxdim: number of neurons in x-direction\nydim: number of neurons in y-direction\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.linearRadius-Tuple{Float64, Float64, Int64, Int64}","page":"Functions","title":"GigaSOM.linearRadius","text":"linearRadius(initRadius::Float64, iteration::Int64, decay::String, epochs::Int64)\n\nReturn a neighbourhood radius. Use as the radiusFun parameter for trainGigaSOM.\n\nArguments\n\ninitRadius: Initial Radius\nfinalRadius: Final Radius\niteration: Training iteration\nepochs: Total number of epochs\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.thresholdKernel","page":"Functions","title":"GigaSOM.thresholdKernel","text":"thresholdKernel(x, r::Float64)\n\nSimple FlowSOM-like hard-threshold kernel\n\n\n\n\n\n","category":"function"},{"location":"functions/#Embedding","page":"Functions","title":"Embedding","text":"","category":"section"},{"location":"functions/","page":"Functions","title":"Functions","text":"Modules = [GigaSOM]\nPages = [\"embedding.jl\"]","category":"page"},{"location":"functions/#GigaSOM.embedGigaSOM-Tuple{Som, Any}","page":"Functions","title":"GigaSOM.embedGigaSOM","text":"embedGigaSOM(som::GigaSOM.Som,\n             data;\n             knnTreeFun = BruteTree,\n             metric = Euclidean(),\n             k::Int64=0,\n             adjust::Float64=1.0,\n             smooth::Float64=0.0,\n             m::Float64=10.0)\n\nOverload of embedGigaSOM for simple DataFrames and matrices. This slices the data using DistributedArrays, sends them the workers, and runs normal embedGigaSOM. All data is properly undistributed after the computation.\n\nExamples:\n\nProduce a 2-column matrix with 2D cell coordinates:\n\ne = embedGigaSOM(som, data)\n\nPlot the result using 2D histogram from Gadfly:\n\nusing Gadfly\ndraw(PNG(\"output.png\",20cm,20cm),\n     plot(x=e[:,1], y=e[:,2],\n     Geom.histogram2d(xbincount=200, ybincount=200)))\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.embedGigaSOM-Tuple{Som, LoadedDataInfo}","page":"Functions","title":"GigaSOM.embedGigaSOM","text":"embedGigaSOM(som::GigaSOM.Som,\n             dInfo::LoadedDataInfo;\n             knnTreeFun = BruteTree,\n             metric = Euclidean(),\n             k::Int64=0,\n             adjust::Float64=1.0,\n             smooth::Float64=0.0,\n             m::Float64=10.0,\n             output::Symbol=tmpSym(dInfo))::LoadedDataInfo\n\nReturn a data frame with X,Y coordinates of EmbedSOM projection of the data.\n\nArguments:\n\nsom: a trained SOM\ndInfo: LoadedDataInfo that describes the loaded dataset\nknnTreeFun: Constructor of the KNN-tree (e.g. from NearestNeighbors package)\nmetric: Passed as metric argument to the KNN-tree constructor\nk: number of nearest neighbors to consider (high values get quadratically slower)\nadjust: position adjustment parameter (higher values avoid non-local approximations)\nsmooth: approximation smoothness (the higher the value, the larger the neighborhood of approximate local linearity of the projection)\nm: exponential decay rate for the score when approaching the k+1-th neighbor distance\noutput: variable name for storing the distributed result\n\nData must have the same number of dimensions as the training dataset, and must be normalized using the same parameters.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.embedGigaSOM_internal-Tuple{Som, Matrix{Float64}, Any, Int64, Float64, Float64, Float64}","page":"Functions","title":"GigaSOM.embedGigaSOM_internal","text":"embedGigaSOM_internal(som::GigaSOM.Som,\n                      data::Matrix{Float64},\n                      tree,\n                      k::Int64,\n                      adjust::Float64,\n                      boost::Float64,\n                      m::Float64)\n\nInternal function to compute parts of the embedding on a prepared kNN-tree structure (tree) and smooth converted to boost.\n\n\n\n\n\n","category":"method"},{"location":"functions/#Distributed-processing-tools","page":"Functions","title":"Distributed processing tools","text":"","category":"section"},{"location":"functions/","page":"Functions","title":"Functions","text":"Modules = [GigaSOM]\nPages = [\"distributed.jl\", \"dio.jl\"]","category":"page"},{"location":"functions/#GigaSOM.distribute_array-Tuple{Symbol, Array, Any}","page":"Functions","title":"GigaSOM.distribute_array","text":"distribute_array(sym, x::Array, pids; dim=1)::LoadedDataInfo\n\nDistribute roughly equal parts of array x separated on dimension dim among pids into a worker-local variable sym.\n\nReturns the LoadedDataInfo structure for the distributed data.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.distribute_darray-Tuple{Symbol, DistributedArrays.DArray}","page":"Functions","title":"GigaSOM.distribute_darray","text":"distribute_darray(sym, dd::DArray)::LoadedDataInfo\n\nDistribute the distributed array parts from dd into worker-local variable sym.\n\nReturns the LoadedDataInfo structure for the distributed data.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.distributed_collect","page":"Functions","title":"GigaSOM.distributed_collect","text":"distributed_collect(dInfo::LoadedDataInfo, dim=1; free=false)\n\nDistributed collect (just as the other overload) that works with LoadedDataInfo.\n\n\n\n\n\n","category":"function"},{"location":"functions/#GigaSOM.distributed_collect-2","page":"Functions","title":"GigaSOM.distributed_collect","text":"distributed_collect(val::Symbol, workers, dim=1; free=false)\n\nCollect the arrays distributed on workers under value val into an array. The individual arrays are pasted in the dimension specified by dim, i.e. dim=1 is roughly equivalent to using vcat, and dim=2 to hcat.\n\nval must be an Array-based type; the function will otherwise fail.\n\nIf free is true, the val is undistributed after collection.\n\nThis preallocates the array for results, and is thus more efficient than e.g. using distributed_mapreduce with vcat for folding.\n\n\n\n\n\n","category":"function"},{"location":"functions/#GigaSOM.distributed_exec-Tuple{Any, Any, Any}","page":"Functions","title":"GigaSOM.distributed_exec","text":"distributed_exec(val, fn, workers)\n\nExecute a function on workers, taking val as a parameter. Results are not collected. This is optimal for various side-effect-causing computations that are not expressible with distributed_transform.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.distributed_exec-Tuple{LoadedDataInfo, Any}","page":"Functions","title":"GigaSOM.distributed_exec","text":"distributed_exec(dInfo::LoadedDataInfo, fn)\n\nVariant of distributed_exec that works with LoadedDataInfo.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.distributed_foreach-Tuple{Vector, Any, Any}","page":"Functions","title":"GigaSOM.distributed_foreach","text":"distributed_foreach(arr::Vector, fn, workers)\n\nCall a function fn on workers, with a single parameter arriving from the corresponding position in arr.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.distributed_mapreduce-NTuple{4, Any}","page":"Functions","title":"GigaSOM.distributed_mapreduce","text":"distributed_mapreduce(val, map, fold, workers)\n\nRun maps (non-modifying transforms on the data) and folds (2-to-1 reductions) on the worker-local data (in vals) distributed on workers and return the final reduced result.\n\nIt is assumed that the fold operation is associative, but not commutative (as in semigroups). If there are no workers, operation returns nothing (we don't have a monoid to magically conjure zero elements :[ ).\n\nIn current version, the reduce step is a sequential left fold, executed in the main process.\n\nExample\n\n# compute the mean of all distributed data\nsum,len = distributed_mapreduce(:myData,\n    (d) -> (sum(d),length(d)),\n    ((s1, l1), (s2, l2)) -> (s1+s2, l1+l2),\n    workers())\nprintln(sum/len)\n\nProcessing multiple arguments (a.k.a. \"zipWith\")\n\nThe val here does not necessarily need to refer to a symbol, you can easily pass in a quoted tuple, which will be unquoted in the function parameter. For example, distributed values :a and :b can be joined as such:\n\ndistributed_mapreduce(:((a,b)),\n    ((a,b)::Tuple) -> [a b],\n    vcat,\n    workers())\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.distributed_mapreduce-Tuple{LoadedDataInfo, Any, Any}","page":"Functions","title":"GigaSOM.distributed_mapreduce","text":"distributed_mapreduce(dInfo::LoadedDataInfo, map, fold)\n\nDistributed map/reduce (just as the other overload of distributed_mapreduce) that works with LoadedDataInfo.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.distributed_mapreduce-Tuple{Vector, Any, Any, Any}","page":"Functions","title":"GigaSOM.distributed_mapreduce","text":"distributed_mapreduce(vals::Vector, map, fold, workers)\n\nVariant of distributed_mapreduce that works with more distributed variables at once.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.distributed_mapreduce-Tuple{Vector{LoadedDataInfo}, Any, Any}","page":"Functions","title":"GigaSOM.distributed_mapreduce","text":"distributed_mapreduce(dInfo1::LoadedDataInfo, dInfo2::LoadedDataInfo, map, fold)\n\nVariant of distributed_mapreduce that works with more LoadedDataInfos at once.  The data must be distributed on the same set of workers, in the same order.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.distributed_transform","page":"Functions","title":"GigaSOM.distributed_transform","text":"distributed_transform(val, fn, workers, tgt::Symbol=val)\n\nTransform the worker-local distributed data available as val on workers in-place, by a function fn. Store the result as tgt (default val)\n\nExample\n\n# multiply all saved data by 2\ndistributed_transform(:myData, (d)->(2*d), workers())\n\n\n\n\n\n","category":"function"},{"location":"functions/#GigaSOM.distributed_transform-2","page":"Functions","title":"GigaSOM.distributed_transform","text":"distributed_transform(dInfo::LoadedDataInfo, fn, tgt::Symbol=dInfo.val)::LoadedDataInfo\n\nSame as distributed_transform, but specialized for LoadedDataInfo.\n\n\n\n\n\n","category":"function"},{"location":"functions/#GigaSOM.get_from-Tuple{Any, Any}","page":"Functions","title":"GigaSOM.get_from","text":"get_from(worker,val)\n\nGet a value val from a remote worker; quoting of val works just as with save_at. Returns a future with the requested value.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.get_val_from-Tuple{Any, Any}","page":"Functions","title":"GigaSOM.get_val_from","text":"get_val_from(worker,val)\n\nShortcut for instantly fetching the future from get_from.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.remove_from-Tuple{Any, Symbol}","page":"Functions","title":"GigaSOM.remove_from","text":"remove_from(worker,sym)\n\nSets symbol sym on worker to nothing, effectively freeing the data.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.save_at-Tuple{Any, Symbol, Any}","page":"Functions","title":"GigaSOM.save_at","text":"save_at(worker, sym, val)\n\nSaves value val to symbol sym at worker. sym should be quoted (or contain a symbol). val gets unquoted in the processing and evaluated at the worker, quote it if you want to pass exact command to the worker.\n\nThis is loosely based on the package ParallelDataTransfers, but made slightly more flexible by omitting/delaying the explicit fetches etc. In particular, save_at is roughly the same as ParallelDataTransfers.sendto, and get_val_from works very much like ParallelDataTransfers.getfrom.\n\nReturn value\n\nA future with Nothing that can be fetched to see that the operation has finished.\n\nExamples\n\naddprocs(1)\nsave_at(2,:x,123)       # saves 123\nsave_at(2,:x,myid())    # saves 1\nsave_at(2,:x,:(myid())) # saves 2\nsave_at(2,:x,:(:x))     # saves the symbol :x\n                        # (just :x won't work because of unquoting)\n\nNote: Symbol scope\n\nThe symbols are saved in Main module on the corresponding worker. For example, save_at(1, :x, nothing) will erase your local x variable. Beware of name collisions.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.tmpSym-Tuple{LoadedDataInfo}","page":"Functions","title":"GigaSOM.tmpSym","text":"tmpSym(dInfo::LoadedDataInfo; prefix=\"\", suffix=\"_tmp\")\n\nDecorate the symbol from dInfo with prefix and suffix.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.tmpSym-Tuple{Symbol}","page":"Functions","title":"GigaSOM.tmpSym","text":"tmpSym(s::Symbol; prefix=\"\", suffix=\"_tmp\")\n\nDecorate a symbol s with prefix and suffix, to create a good name for a related temporary value.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.undistribute-Tuple{LoadedDataInfo}","page":"Functions","title":"GigaSOM.undistribute","text":"undistribute(dInfo::LoadedDataInfo)\n\nRemove the loaded data described by dInfo from the corresponding workers.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.undistribute-Tuple{Symbol, Any}","page":"Functions","title":"GigaSOM.undistribute","text":"undistribute(sym, workers)\n\nRemove the loaded data from workers.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.defaultFiles-Tuple{Any, Any}","page":"Functions","title":"GigaSOM.defaultFiles","text":"defaultFiles(s, pids)\n\nMake a good set of filenames for saving a dataset.\n\n\n\n\n\n","category":"method"},{"location":"functions/#GigaSOM.distributed_export","page":"Functions","title":"GigaSOM.distributed_export","text":"distributed_export(dInfo::LoadedDataInfo, files=defaultFiles(dInfo.val, dInfo.workers))\n\nOverloaded functionality for LoadedDataInfo.\n\n\n\n\n\n","category":"function"},{"location":"functions/#GigaSOM.distributed_export-2","page":"Functions","title":"GigaSOM.distributed_export","text":"distributed_export(sym::Symbol, pids, files=defaultFiles(sym,pids))\n\nExport the content of symbol sym by each worker specified by pids to a corresponding filename in files.\n\n\n\n\n\n","category":"function"},{"location":"functions/#GigaSOM.distributed_import","page":"Functions","title":"GigaSOM.distributed_import","text":"distributed_import(dInfo::LoadedDataInfo, files=defaultFiles(dInfo.val, dInfo.workers))\n\nOverloaded functionality for LoadedDataInfo.\n\n\n\n\n\n","category":"function"},{"location":"functions/#GigaSOM.distributed_import-2","page":"Functions","title":"GigaSOM.distributed_import","text":"distributed_import(sym::Symbol, pids, files=defaultFiles(sym,pids))\n\nImport the content of symbol sym by each worker specified by pids from the corresponding filename in files.\n\n\n\n\n\n","category":"function"},{"location":"functions/#GigaSOM.distributed_unlink","page":"Functions","title":"GigaSOM.distributed_unlink","text":"distributed_unlink(dInfo::LoadedDataInfo, files=defaultFiles(dInfo.val, dInfo.workers))\n\nOverloaded functionality for LoadedDataInfo.\n\n\n\n\n\n","category":"function"},{"location":"functions/#GigaSOM.distributed_unlink-2","page":"Functions","title":"GigaSOM.distributed_unlink","text":"distributed_unlink(sym::Symbol, pids, files=defaultFiles(sym,pids))\n\nRemove the files created by distributed_export with the same parameters.\n\n\n\n\n\n","category":"function"},{"location":"tutorials/whereToGoNext/#Where-to-continue-after-finishing-the-tutorials?","page":"Conclusion","title":"Where to continue after finishing the tutorials?","text":"","category":"section"},{"location":"tutorials/whereToGoNext/","page":"Conclusion","title":"Conclusion","text":"After reading through the tutorials, you should now have a decent idea of how GigaSOM.jl works internally. With real data, you may encounter situations that require deeper digging in parameters and GigaSOM internals. We list several of the most frequently used ones:","category":"page"},{"location":"tutorials/whereToGoNext/","page":"Conclusion","title":"Conclusion","text":"You may want to increase the size of SOM in initGigaSOM to get more precise clusters.\nYou may speed up the computation a lot by using neighborhood-indexing structures – see package NearestNeighbors and parameters knnTreeFun of functions trainGigaSOM and embedGigaSOM\nIt is adviced to try different settings of SOM training – of the arguments of trainGigaSOM, try modifying the starting/finishing radius (rStart, rFinal), using a different radius decay (parameter radiusFun, try e.g. linearRadius) or try a different neighborhood (kernelFun and somDistFun) or a completely different metric.\nYou can get a sharper or smoother embedding by varying the amount of neighbors (k) and smoothing of the neighborhood (smooth) of embedGigaSOM.\nFor plotting of really huge data, you may want to try GigaScatter.","category":"page"},{"location":"tutorials/processingFCSData/#Tutorial-2:-Working-with-cytometry-data","page":"Cytometry data","title":"Tutorial 2: Working with cytometry data","text":"","category":"section"},{"location":"tutorials/processingFCSData/","page":"Cytometry data","title":"Cytometry data","text":"You can load any FCS file using loadFCS function. For example, the Levine dataset (obtainable here) may be loaded as such:","category":"page"},{"location":"tutorials/processingFCSData/","page":"Cytometry data","title":"Cytometry data","text":"params, data = loadFCS(\"Levine_13dim.fcs\")","category":"page"},{"location":"tutorials/processingFCSData/","page":"Cytometry data","title":"Cytometry data","text":"params will now contain the list of FCS parameters; you can parse a lot of interesting information from it using the getMetaData function:","category":"page"},{"location":"tutorials/processingFCSData/","page":"Cytometry data","title":"Cytometry data","text":"getMetaData(params)","category":"page"},{"location":"tutorials/processingFCSData/","page":"Cytometry data","title":"Cytometry data","text":"14×8 DataFrames.DataFrame. Omitted printing of 3 columns\n│ Row │ E      │ S      │ N      │ RMIN              │ R      │\n│     │ String │ String │ String │ String            │ String │\n├─────┼────────┼────────┼────────┼───────────────────┼────────┤\n│ 1   │ 0,0    │        │ CD45   │ -2.03601189282714 │ 1024   │\n│ 2   │ 0,0    │        │ CD45RA │ -2.99700270621007 │ 1024   │\n│ 3   │ 0,0    │        │ CD19   │ -3.05850183816765 │ 1024   │\n│ 4   │ 0,0    │        │ CD11b  │ -2.99956408593931 │ 1024   │\n│ 5   │ 0,0    │        │ CD4    │ -2.22860674361335 │ 1024   │\n│ 6   │ 0,0    │        │ CD8    │ -3.29174106765763 │ 1024   │\n│ 7   │ 0,0    │        │ CD34   │ -2.74278770893026 │ 1024   │\n│ 8   │ 0,0    │        │ CD20   │ -3.40866348184011 │ 1024   │\n│ 9   │ 0,0    │        │ CD33   │ -2.31371406643428 │ 1024   │\n│ 10  │ 0,0    │        │ CD123  │ -3.02624638359366 │ 1024   │\n│ 11  │ 0,0    │        │ CD38   │ -3.14752313833461 │ 1024   │\n│ 12  │ 0,0    │        │ CD90   │ -2.55305031846157 │ 1024   │\n│ 13  │ 0,0    │        │ CD3    │ -3.52459385416266 │ 1024   │\n│ 14  │ 0,0    │        │ label  │ 1                 │ 1024   │","category":"page"},{"location":"tutorials/processingFCSData/","page":"Cytometry data","title":"Cytometry data","text":"data is a matrix with cell expressions, one cell per row, one marker per column. If you want to run SOM analysis on it, you can cluster and visualize it just as in the previous tutorial, with one exception- we start with cutting off the label column that contains NaN values:","category":"page"},{"location":"tutorials/processingFCSData/","page":"Cytometry data","title":"Cytometry data","text":"\ndata = data[:,1:13]\nsom = initGigaSOM(data, 16, 16)\nsom = trainGigaSOM(som, data)\nclusters = mapToGigaSOM(som, data)\ne = embedGigaSOM(som, data)\n\n# ... save/plot results, etc...","category":"page"},{"location":"tutorials/processingFCSData/#Work-with-distributed-data","page":"Cytometry data","title":"Work with distributed data","text":"","category":"section"},{"location":"tutorials/processingFCSData/","page":"Cytometry data","title":"Cytometry data","text":"Usual experiments produce multiple FCS files, and distributed or parallel processing is very helpful in crunching through all the data.","category":"page"},{"location":"tutorials/processingFCSData/","page":"Cytometry data","title":"Cytometry data","text":"To load multiple FCS files, use loadFCSSet. This function works well in the \"usual\" single-process environment, but additionally it is designed to handle situations when the data is too big to fit into memory, and attempts to split them into available distributed workers workers.","category":"page"},{"location":"tutorials/processingFCSData/","page":"Cytometry data","title":"Cytometry data","text":"For the purpose of data distribution, you need to identify each dataset by an unique dataset name that will be used for identifying your loaded data in the cluster environment.  The dataset name is a simple Julia symbols; basically a variable name that is prefixed with a : colon.","category":"page"},{"location":"tutorials/processingFCSData/","page":"Cytometry data","title":"Cytometry data","text":"For example, we can load the Levine13 dataset as such:","category":"page"},{"location":"tutorials/processingFCSData/","page":"Cytometry data","title":"Cytometry data","text":"datainfo = loadFCSSet(:levine, [\"Levine_13dim.fcs\"])","category":"page"},{"location":"tutorials/processingFCSData/","page":"Cytometry data","title":"Cytometry data","text":"Expectably, if you have more files, just write their names into the array and the function will handle the rest.","category":"page"},{"location":"tutorials/processingFCSData/","page":"Cytometry data","title":"Cytometry data","text":"The result datainfo carries informaton about your selected dataset name and its distribution among the cluster. It can be used just as the \"data\" parameter in all SOM-related functions again; e.g. as trainGigaSOM(som, datainfo).","category":"page"},{"location":"tutorials/processingFCSData/","page":"Cytometry data","title":"Cytometry data","text":"The following example exploits the possibility to actually split the data, and processes the Levine dataset parallelly on 2 workers:","category":"page"},{"location":"tutorials/processingFCSData/","page":"Cytometry data","title":"Cytometry data","text":"using Distributed\naddprocs(2)                 # add any number of CPUs/tasks/workers you have available\n@everywhere using GigaSOM   # load GigaSOM also on the workers\n\ndatainfo = loadFCSSet(:levine, [\"Levine_13dim.fcs\"]) # add more files as needed\n\ndselect(datainfo, Vector(1:13))   # select columns that contain expressions (column 14 contains labels)\nsom = initGigaSOM(datainfo, 20, 20)\nsom = trainGigaSOM(som, datainfo)","category":"page"},{"location":"tutorials/processingFCSData/","page":"Cytometry data","title":"Cytometry data","text":"To prevent memory overload of the \"master\" computation node, the results of all per-cell operations are also stored in distributed datainfo objects. In this case, the following code does the embedding, but leaves the resulting data safely scattered among the cluster:","category":"page"},{"location":"tutorials/processingFCSData/","page":"Cytometry data","title":"Cytometry data","text":"e = embedGigaSOM(som, datainfo)","category":"page"},{"location":"tutorials/processingFCSData/","page":"Cytometry data","title":"Cytometry data","text":"If you are sure you have enough RAM, you can collect the data to the master node. (In case of the relatively small Levine13 dataset, you very probably have the required 2.5MB of RAM, but there are many larger datasets.)","category":"page"},{"location":"tutorials/processingFCSData/","page":"Cytometry data","title":"Cytometry data","text":"e = distributed_collect(e)","category":"page"},{"location":"tutorials/processingFCSData/","page":"Cytometry data","title":"Cytometry data","text":"167044×2 Array{Float64,2}:\n 16.8251   11.5002\n 18.2608   12.884\n 12.0103    5.18401\n 18.381    12.3436\n 18.357    11.6622\n 14.8936   12.0897\n 17.6441   12.3652\n 17.8752   12.7206\n 17.301    11.0767\n 14.2055   12.2227\n  ⋮","category":"page"},{"location":"tutorials/processingFCSData/#Working-with-larger-datasets","page":"Cytometry data","title":"Working with larger datasets","text":"","category":"section"},{"location":"tutorials/processingFCSData/","page":"Cytometry data","title":"Cytometry data","text":"In this example we will use a subset of the Cytometry data by Bodenmiller et al.. This data-set contains samples from peripheral blood mononuclear cells (PBMCs) in unstimulated and stimulated conditions for 8 healthy donors.","category":"page"},{"location":"tutorials/processingFCSData/","page":"Cytometry data","title":"Cytometry data","text":"10 cell surface markers (lineage markers) are used to identify different cell populations. The dataset is described in two files:","category":"page"},{"location":"tutorials/processingFCSData/","page":"Cytometry data","title":"Cytometry data","text":"PBMC8_panel.xlsx (with antigen names categorized as lineage markers and functional markers)\nPBMC8_metadata.xlsx (file names, sample IDs, condition IDs and patient IDs)","category":"page"},{"location":"tutorials/processingFCSData/#Download-and-prepare-the-dataset","page":"Cytometry data","title":"Download and prepare the dataset","text":"","category":"section"},{"location":"tutorials/processingFCSData/","page":"Cytometry data","title":"Cytometry data","text":"The example data can be downloaded from imlspenticton.uzh.ch/robinson_lab/cytofWorkflow/","category":"page"},{"location":"tutorials/processingFCSData/","page":"Cytometry data","title":"Cytometry data","text":"You can fetch the files directly from within Julia:","category":"page"},{"location":"tutorials/processingFCSData/","page":"Cytometry data","title":"Cytometry data","text":"# fetch the required data for testing and download the zip archive and unzip it\ndataFiles = [\"PBMC8_metadata.xlsx\", \"PBMC8_panel.xlsx\", \"PBMC8_fcs_files.zip\"]\nfor f in dataFiles\n    if !isfile(f)\n        download(\"http://imlspenticton.uzh.ch/robinson_lab/cytofWorkflow/\"*f, f)\n        if occursin(\".zip\", f)\n            run(`unzip PBMC8_fcs_files.zip`)\n        end\n    end\nend","category":"page"},{"location":"tutorials/processingFCSData/","page":"Cytometry data","title":"Cytometry data","text":"The metadata is present in external files; we read it into a DataFrame and extract information about FCS data columns from there. First, we read the actual content using the XLSX package:","category":"page"},{"location":"tutorials/processingFCSData/","page":"Cytometry data","title":"Cytometry data","text":"using XLSX\nmd = GigaSOM.DataFrame(readtable(\"PBMC8_metadata.xlsx\", \"Sheet1\", infer_eltypes=true)...)\npanel = GigaSOM.DataFrame(readtable(\"PBMC8_panel.xlsx\", \"Sheet1\", infer_eltypes=true)...)","category":"page"},{"location":"tutorials/processingFCSData/","page":"Cytometry data","title":"Cytometry data","text":"After that, we can get the parameter structure from the first FCS files:","category":"page"},{"location":"tutorials/processingFCSData/","page":"Cytometry data","title":"Cytometry data","text":"_, fcsParams = loadFCSHeader(md[1, :file_name])","category":"page"},{"location":"tutorials/processingFCSData/","page":"Cytometry data","title":"Cytometry data","text":"Continue with extracting marker names using the prepared functions:","category":"page"},{"location":"tutorials/processingFCSData/","page":"Cytometry data","title":"Cytometry data","text":"_, fcsAntigens = getMarkerNames(getMetaData(fcsParams))","category":"page"},{"location":"tutorials/processingFCSData/","page":"Cytometry data","title":"Cytometry data","text":"Now, see which antigens we want to use (assume we want only the lineage markers):","category":"page"},{"location":"tutorials/processingFCSData/","page":"Cytometry data","title":"Cytometry data","text":"antigens = panel[panel[:,:Lineage].==1, :Antigen]","category":"page"},{"location":"tutorials/processingFCSData/","page":"Cytometry data","title":"Cytometry data","text":"Finally, it is often useful to make the names a bit more Julia-friendly and predictable:","category":"page"},{"location":"tutorials/processingFCSData/","page":"Cytometry data","title":"Cytometry data","text":"cleanNames!(antigens)\ncleanNames!(fcsAntigens)","category":"page"},{"location":"tutorials/processingFCSData/#Load-and-prepare-the-data","page":"Cytometry data","title":"Load and prepare the data","text":"","category":"section"},{"location":"tutorials/processingFCSData/","page":"Cytometry data","title":"Cytometry data","text":"Now we have the vector of fcsAntigens that the FCS files store, and list of antigens that we want to analyze. We continue by loading the data, reducing it to the desired antigens and transforming it a bit:","category":"page"},{"location":"tutorials/processingFCSData/","page":"Cytometry data","title":"Cytometry data","text":"di = loadFCSSet(:pbmc8, md[:,:file_name])","category":"page"},{"location":"tutorials/processingFCSData/","page":"Cytometry data","title":"Cytometry data","text":"(If data distribution and parallelization is required, you must add parallel workers using addprocs before this step.)","category":"page"},{"location":"tutorials/processingFCSData/","page":"Cytometry data","title":"Cytometry data","text":"Now that the data is loaded, let's prepare them a bit by reducing to actual interesting columns, transformation and scaling:","category":"page"},{"location":"tutorials/processingFCSData/","page":"Cytometry data","title":"Cytometry data","text":"# select only the columns that correspond to the lineage antigens we have prepared before\ndselect(di, fcsAntigens, antigens)\n\ncols = Vector(1:length(antigens)) # shortcut for \"all rows\"\n\n# perform asinh transformation on all data in the dataset, '5' is the cofactor for the transformation\ndtransform_asinh(di, cols, 5)\n\n# normalize all dataset columns to mean=0 sdev=1\ndscale(di, cols)","category":"page"},{"location":"tutorials/processingFCSData/#Create-a-Self-Organizing-MAP-(SOM)","page":"Cytometry data","title":"Create a Self Organizing MAP (SOM)","text":"","category":"section"},{"location":"tutorials/processingFCSData/","page":"Cytometry data","title":"Cytometry data","text":"With the data prepared, running the SOM algorithm is straightforward:","category":"page"},{"location":"tutorials/processingFCSData/","page":"Cytometry data","title":"Cytometry data","text":"# randomly initialize the SOM\nsom = initGigaSOM(di, 16, 16)\n\n# train the SOM for 20 epochs (10 is default, but nothing will happen if the\n# epochs are slightly overdone)\nsom = trainGigaSOM(som, di, epochs = 20)","category":"page"},{"location":"tutorials/processingFCSData/","page":"Cytometry data","title":"Cytometry data","text":"Finally, calculate the clustering:","category":"page"},{"location":"tutorials/processingFCSData/","page":"Cytometry data","title":"Cytometry data","text":"somClusters = mapToGigaSOM(som, di)","category":"page"},{"location":"tutorials/processingFCSData/#FlowSOM-style-metaclustering","page":"Cytometry data","title":"FlowSOM-style metaclustering","text":"","category":"section"},{"location":"tutorials/processingFCSData/","page":"Cytometry data","title":"Cytometry data","text":"One disadvantage of SOMs is that they output a large amount of small clusters that are relatively hard to interpret manually. FlowSOM improved that situation by running a \"clustering on clusters\" (metaclustering) that address the problem.","category":"page"},{"location":"tutorials/processingFCSData/","page":"Cytometry data","title":"Cytometry data","text":"In this example, we reduce the original 256 small clusters from 16x16 SOM to only 10 \"metaclusters\", using the standard hierarchical clustering:","category":"page"},{"location":"tutorials/processingFCSData/","page":"Cytometry data","title":"Cytometry data","text":"using Clustering\nimport Distances\nmetaClusters =\n  cutree(k=10,\n         hclust(linkage=:average,\n            GigaSOM.distMatrix(Distances.Euclidean())(som.codes)))","category":"page"},{"location":"tutorials/processingFCSData/","page":"Cytometry data","title":"Cytometry data","text":"The metaClusters represent membership of the SOM codes in cluster; these can be expanded to membership of all cells using mapToGigaSOM:","category":"page"},{"location":"tutorials/processingFCSData/","page":"Cytometry data","title":"Cytometry data","text":"mapping = distributed_collect(mapToGigaSOM(som, di), free=true)\nclusters = metaClusters[mapping]","category":"page"},{"location":"tutorials/processingFCSData/","page":"Cytometry data","title":"Cytometry data","text":"clusters now contain an integer from 1 to 10 with a classification of each cell in the dataset.","category":"page"},{"location":"tutorials/processingFCSData/","page":"Cytometry data","title":"Cytometry data","text":"(The argument free=true of distributed_collect automatically removes the distributed data from workers after collecting, which saves their memory for other datasets.)","category":"page"},{"location":"howToContribute/#How-to-contribute-to/develop-GigaSOM","page":"How to contribute","title":"How to contribute to/develop GigaSOM","text":"","category":"section"},{"location":"howToContribute/","page":"How to contribute","title":"How to contribute","text":"If you want to contribute to the GigaSOM package, please fork the present repository by following these instructions.","category":"page"},{"location":"howToContribute/#Step-1:-Retrieve-a-local-version-of-GigaSOM","page":"How to contribute","title":"Step 1: Retrieve a local version of GigaSOM","text":"","category":"section"},{"location":"howToContribute/","page":"How to contribute","title":"How to contribute","text":"There are two ways that you can retrieve a local copy of the package: one is to manually clone the forked repository, and the second one is to use the intergrated Julia package manager.","category":"page"},{"location":"howToContribute/#Option-1:-Manually-clone-your-fork","page":"How to contribute","title":"Option 1: Manually clone your fork","text":"","category":"section"},{"location":"howToContribute/","page":"How to contribute","title":"How to contribute","text":":warning: Please make sure you have forked the repository, as described above.","category":"page"},{"location":"howToContribute/","page":"How to contribute","title":"How to contribute","text":"You can do this as follows from the command line:","category":"page"},{"location":"howToContribute/","page":"How to contribute","title":"How to contribute","text":"$ git clone git@github.com:yourUsername/GigaSOM.jl.git GigaSOM.jl\n$ cd GigaSOM.jl\n$ git checkout -b yourNewBranch origin/develop","category":"page"},{"location":"howToContribute/","page":"How to contribute","title":"How to contribute","text":"where yourUsername is your Github username and yourNewBranch is the name of a new branch.","category":"page"},{"location":"howToContribute/","page":"How to contribute","title":"How to contribute","text":"Then, in order to develop the package, you can install your cloned version as follows (make sure you are in the GigaSOM.jl directory):","category":"page"},{"location":"howToContribute/","page":"How to contribute","title":"How to contribute","text":"(v1.1) pkg> add .","category":"page"},{"location":"howToContribute/","page":"How to contribute","title":"How to contribute","text":"(press ] to get into the packaging environment)","category":"page"},{"location":"howToContribute/","page":"How to contribute","title":"How to contribute","text":"This adds the GigaSOM.jl package and all its dependencies. You can verify that the installation worked by typing:","category":"page"},{"location":"howToContribute/","page":"How to contribute","title":"How to contribute","text":"(v1.1) pkg> status","category":"page"},{"location":"howToContribute/","page":"How to contribute","title":"How to contribute","text":"If everything went smoothly, this should print something similar to:","category":"page"},{"location":"howToContribute/","page":"How to contribute","title":"How to contribute","text":"(v1.1) pkg> status\n    Status `~/.julia/environments/v1.1/Project.toml`\n  [a03a9c34] GigaSOM v0.0.5 #yourNewBranch (.)","category":"page"},{"location":"howToContribute/","page":"How to contribute","title":"How to contribute","text":"Now, you can readily start using the GigaSOM module:","category":"page"},{"location":"howToContribute/","page":"How to contribute","title":"How to contribute","text":"julia> using GigaSOM","category":"page"},{"location":"howToContribute/#Option-2:-Use-the-Julia-package-manager","page":"How to contribute","title":"Option 2: Use the Julia package manager","text":"","category":"section"},{"location":"howToContribute/","page":"How to contribute","title":"How to contribute","text":"When you are used to using the Julia package manager for developing or contributing to packages, you can type:","category":"page"},{"location":"howToContribute/","page":"How to contribute","title":"How to contribute","text":"(v1.1) pkg> dev GigaSOM","category":"page"},{"location":"howToContribute/","page":"How to contribute","title":"How to contribute","text":"This will install the GigaSOM package locally and check it out for development. You can check the location of the package with:","category":"page"},{"location":"howToContribute/","page":"How to contribute","title":"How to contribute","text":"(v1.1) pkg> status\n    Status `~/.julia/environments/v1.1/Project.toml`\n  [a03a9c34] GigaSOM v0.0.5 [`~/.julia/dev/GigaSOM`]","category":"page"},{"location":"howToContribute/","page":"How to contribute","title":"How to contribute","text":"The default location of the package is ~/.julia/dev/GigaSOM.","category":"page"},{"location":"howToContribute/","page":"How to contribute","title":"How to contribute","text":"You can then set your remote by executing these commands in a regular shell:","category":"page"},{"location":"howToContribute/","page":"How to contribute","title":"How to contribute","text":"$ cd ~/.julia/dev/GigaSOM\n$ git remote rename origin upstream # renames the origin as upstream\n$ git remote add origin git@github.com:yourUsername/GigaSOM.jl.git\n$ git fetch origin","category":"page"},{"location":"howToContribute/","page":"How to contribute","title":"How to contribute","text":"where yourUsername is your Github username.","category":"page"},{"location":"howToContribute/","page":"How to contribute","title":"How to contribute","text":":warning: Make sure that your fork exists under github.com/yourUsername/GigaSOM.jl.","category":"page"},{"location":"howToContribute/","page":"How to contribute","title":"How to contribute","text":"Then, checkout a branch yourNewBranch:","category":"page"},{"location":"howToContribute/","page":"How to contribute","title":"How to contribute","text":"$ cd ~/.julia/dev/GigaSOM\n$ git checkout -b yourNewBranch origin/develop","category":"page"},{"location":"howToContribute/","page":"How to contribute","title":"How to contribute","text":"Then, you can readily use the GigaSOM package:","category":"page"},{"location":"howToContribute/","page":"How to contribute","title":"How to contribute","text":"julia> using GigaSOM","category":"page"},{"location":"howToContribute/","page":"How to contribute","title":"How to contribute","text":"After making changes, precompile the package:","category":"page"},{"location":"howToContribute/","page":"How to contribute","title":"How to contribute","text":"(v1.1) pkg> precompile","category":"page"},{"location":"howToContribute/#Step-2:-Activate-GigaSOM","page":"How to contribute","title":"Step 2: Activate GigaSOM","text":"","category":"section"},{"location":"howToContribute/","page":"How to contribute","title":"How to contribute","text":":warning: Please note that you cannot use the dependencies of GigaSOM directly, unless they are installed separately or the environment has been activated:","category":"page"},{"location":"howToContribute/","page":"How to contribute","title":"How to contribute","text":"(v1.1) pkg> activate .\n(GigaSOM) pkg> instantiate","category":"page"},{"location":"howToContribute/","page":"How to contribute","title":"How to contribute","text":"Now, the environment is activated (you can see it with the prompt change (GigaSOM) pkg>). Now, you can use the dependency. For instance:","category":"page"},{"location":"howToContribute/","page":"How to contribute","title":"How to contribute","text":"julia> using DataFrames","category":"page"},{"location":"howToContribute/","page":"How to contribute","title":"How to contribute","text":":warning: If you do not  activate the environment before using any of the dependencies, you will see a red error messages prompting you to install the dependency explicity.","category":"page"},{"location":"tutorials/basicUsage/#Tutorial-1:-Intro-and-basic-usage","page":"Introduction","title":"Tutorial 1: Intro & basic usage","text":"","category":"section"},{"location":"tutorials/basicUsage/","page":"Introduction","title":"Introduction","text":"For the installation of Julia or GigaSOM.jl please refer to the installation instructions.","category":"page"},{"location":"tutorials/basicUsage/#High-level-overview","page":"Introduction","title":"High-level overview","text":"","category":"section"},{"location":"tutorials/basicUsage/","page":"Introduction","title":"Introduction","text":"GigaSOM provides functions that allow straightforward loading of FCS files into matrices, preparing these matrices for analysis, and running SOM and related function on the data.","category":"page"},{"location":"tutorials/basicUsage/","page":"Introduction","title":"Introduction","text":"The main functions, listed by category:","category":"page"},{"location":"tutorials/basicUsage/","page":"Introduction","title":"Introduction","text":"loadFCS and loadFCSSet for loading the data\ngetMetaData and getMarkerNames for accessing the information about data columns stored in FCS files\ndselect, dtransform_asinh, dscale and similar functions for transforming, scaling and preparing the data\ninitGigaSOM for initializing the self-organizing map, trainGigaSOM for running the SOM training, mapToGigaSOM for classification using the trained SOM, and embedGigaSOM for dimensionality reduction to 2D","category":"page"},{"location":"tutorials/basicUsage/","page":"Introduction","title":"Introduction","text":"Multiprocessing is done using the Distributed package – if you add more \"workers\" using the addprocs function, GigaSOM will automatically react to that situation, split the data among the processes and run parallel versions of all algorithms.","category":"page"},{"location":"tutorials/basicUsage/#Horizontal-scaling","page":"Introduction","title":"Horizontal scaling","text":"","category":"section"},{"location":"tutorials/basicUsage/","page":"Introduction","title":"Introduction","text":"While all functions work well on simple data matrices, the main aim of GigaSOM is to let the users enjoy the cluster-computing resources. All functions also work on data that are \"scattered\" among workers (i.e. each worker only holds a portion of the data loaded in the memory). This dataset description is stored in LoadedDataInfo structure. Most of the functions above in fact accept the LoadedDataInfo as argument, and often return another LoadedDataInfo that describes the scattered result.","category":"page"},{"location":"tutorials/basicUsage/","page":"Introduction","title":"Introduction","text":"Most importantly, using LoadedDataInfo prevents memory exhaustion at the master node, which is a critical feature required to handle huge datasets.","category":"page"},{"location":"tutorials/basicUsage/","page":"Introduction","title":"Introduction","text":"You can always collect the scattered data back into a matrix (if it fits to your RAM) with distributed_collect, and utilize many other functions to manipulate it, including e.g. distributed_mapreduce for easily running parallel computations, or distributed_export for saving and restoring the dataset paralelly.","category":"page"},{"location":"tutorials/basicUsage/#Minimal-working-example","page":"Introduction","title":"Minimal working example","text":"","category":"section"},{"location":"tutorials/basicUsage/","page":"Introduction","title":"Introduction","text":"First, load GigaSOM:","category":"page"},{"location":"tutorials/basicUsage/","page":"Introduction","title":"Introduction","text":"using GigaSOM","category":"page"},{"location":"tutorials/basicUsage/","page":"Introduction","title":"Introduction","text":"We will create a bit of randomly generated data for this purpose. This code generates a 4D hypercube of size 10 with gaussian clusters at vertices:","category":"page"},{"location":"tutorials/basicUsage/","page":"Introduction","title":"Introduction","text":"d = randn(10000,4) .+ rand(0:1, 10000, 4).*10","category":"page"},{"location":"tutorials/basicUsage/","page":"Introduction","title":"Introduction","text":"The SOM (of size 20x20) is created and trained as such:","category":"page"},{"location":"tutorials/basicUsage/","page":"Introduction","title":"Introduction","text":"som = initGigaSOM(d, 20, 20)\nsom = trainGigaSOM(som, d)","category":"page"},{"location":"tutorials/basicUsage/","page":"Introduction","title":"Introduction","text":"(Note that SOM initialization is randomized; if you want to get the same results everytime, use e.g. Random.seed!(1).)","category":"page"},{"location":"tutorials/basicUsage/","page":"Introduction","title":"Introduction","text":"You can now see the SOM codebook (your numbers will vary):","category":"page"},{"location":"tutorials/basicUsage/","page":"Introduction","title":"Introduction","text":"som.codes","category":"page"},{"location":"tutorials/basicUsage/","page":"Introduction","title":"Introduction","text":"400×4 Array{Float64,2}:\n -0.361681    -0.57191     0.140438   9.99224\n -1.111        1.60277    -0.209706   9.96805\n -1.23305      7.58148    -0.445886   9.76316\n -0.285692     9.80184    -1.12107    9.85507\n -0.197007    10.8793     -0.649294   9.89448\n -0.334737    11.0858      0.213889   9.93479\n  0.00282155  11.0725      0.718114  10.2714\n -0.333398    10.1315      1.14412   10.564\n -0.0124202    1.48128     8.35741   10.72\n -0.0084074    0.0150858   9.91007   11.5361\n  ⋮","category":"page"},{"location":"tutorials/basicUsage/","page":"Introduction","title":"Introduction","text":"This information can be used to categorize the dataset into clusters:","category":"page"},{"location":"tutorials/basicUsage/","page":"Introduction","title":"Introduction","text":"mapToGigaSOM(som, d)","category":"page"},{"location":"tutorials/basicUsage/","page":"Introduction","title":"Introduction","text":"In the result, index is a cluster ID for the original datapoint from d at the same row.","category":"page"},{"location":"tutorials/basicUsage/","page":"Introduction","title":"Introduction","text":"10000×1 DataFrames.DataFrame\n│ Row   │ index │\n│       │ Int64 │\n├───────┼───────┤\n│ 1     │ 381   │\n│ 2     │ 178   │\n│ 3     │ 348   │\n│ 4     │ 379   │\n│ 5     │ 80    │\n│ 6     │ 146   │\n│ 7     │ 57    │\n⋮","category":"page"},{"location":"tutorials/basicUsage/","page":"Introduction","title":"Introduction","text":"(As in the previous case, your numbers may differ.)","category":"page"},{"location":"tutorials/basicUsage/","page":"Introduction","title":"Introduction","text":"Finally, you can use EmbedSOM dimensionality reduction to convert all multidimensional points to 2D; which can eventually be used to create a good-looking 2D scatterplot.","category":"page"},{"location":"tutorials/basicUsage/","page":"Introduction","title":"Introduction","text":"e = embedGigaSOM(som,d)","category":"page"},{"location":"tutorials/basicUsage/","page":"Introduction","title":"Introduction","text":"10000×2 Array{Float64,2}:\n  1.41575  18.5282\n 17.4483    7.4137\n  6.88243  17.5722\n 17.654    18.0348\n 17.594     3.15645\n  5.27181   8.61096\n 15.9708    2.8124\n  6.19637   9.05302\n  1.49358   7.19198\n 16.596     7.75608\n  ⋮","category":"page"},{"location":"tutorials/basicUsage/","page":"Introduction","title":"Introduction","text":"The 2D coordinates may be plotted by any standard plotting library. In the following example we show how to do that with Gadfly:","category":"page"},{"location":"tutorials/basicUsage/","page":"Introduction","title":"Introduction","text":"Pkg.add(\"Gadfly\")\nPkg.add(\"Cairo\")\nusing Gadfly\nimport Cairo\ndraw(PNG(\"test.png\",20cm,20cm), plot(x=e[:,1], y=e[:,2], color=d[:,1]))","category":"page"},{"location":"tutorials/basicUsage/","page":"Introduction","title":"Introduction","text":"In the resulting picture, you should be able to see all 16 gaussian clusters colored by the first dimension in the original space.","category":"page"},{"location":"tutorials/distributedProcessing/#Tutorial-3:-Distributed-data-processing-and-statistics","page":"Advanced distributed processing","title":"Tutorial 3: Distributed data processing and statistics","text":"","category":"section"},{"location":"tutorials/distributedProcessing/","page":"Advanced distributed processing","title":"Advanced distributed processing","text":"If you can get the data on a single machine, computation of various statistics can be performed using standard Julia functions. With large datasets that do not fit on a computer, things get more complicated. Luckily, many statistics and algorithms possess parallel, map-reduce-style implementations that can be used to address this problem.","category":"page"},{"location":"tutorials/distributedProcessing/","page":"Advanced distributed processing","title":"Advanced distributed processing","text":"For example, the dstat function computes sample statistics without fetching all data to a single node. You can use it in a way similar to aforementioned dselect and dtransform_asinh. The following code extracts means and standard deviations from the first 3 columns of a dataset distributed as di:","category":"page"},{"location":"tutorials/distributedProcessing/","page":"Advanced distributed processing","title":"Advanced distributed processing","text":"dstat(di, [1,2,3])","category":"page"},{"location":"tutorials/distributedProcessing/#Manual-work-with-the-distributed-data","page":"Advanced distributed processing","title":"Manual work with the distributed data","text":"","category":"section"},{"location":"tutorials/distributedProcessing/","page":"Advanced distributed processing","title":"Advanced distributed processing","text":"We will first show how to use the general framework to compute per-cluster statistics. GigaSOM exports the distributed_mapreduce function that can be used as a very effective basic building block for running such computations. For example, you can efficiently compute a distributed mean of all your data as such:","category":"page"},{"location":"tutorials/distributedProcessing/","page":"Advanced distributed processing","title":"Advanced distributed processing","text":"distributed_mapreduce(di, sum, +) / distributed_mapreduce(di, length, +)","category":"page"},{"location":"tutorials/distributedProcessing/","page":"Advanced distributed processing","title":"Advanced distributed processing","text":"The parameters of distributed_mapreduce are, in order:","category":"page"},{"location":"tutorials/distributedProcessing/","page":"Advanced distributed processing","title":"Advanced distributed processing","text":"di, the dataset\nsum or length, an unary \"map\" function – during the computation, each piece of distributed data is first paralelly processed by this function\n+, a binary \"reduction\" or \"folding\" function – the pieces of information processed by the map function are successively joined in pairs using this function, until there is only a single result left. This final result is also what distributed_mapreduce returns.","category":"page"},{"location":"tutorials/distributedProcessing/","page":"Advanced distributed processing","title":"Advanced distributed processing","text":"Above example thus reads: Sum all data on all workers, add up the intermediate results, and divide the final number to the sum of all lengths of data on the workers.","category":"page"},{"location":"tutorials/distributedProcessing/","page":"Advanced distributed processing","title":"Advanced distributed processing","text":"Column-wise mean (as produced by dstat) is slightly more useful; we only need to split the computation on columns:","category":"page"},{"location":"tutorials/distributedProcessing/","page":"Advanced distributed processing","title":"Advanced distributed processing","text":"distributed_mapreduce(di, d -> mapslices(sum, d, dims=1), +) ./ distributed_mapreduce(di, x->size(x,1), +)","category":"page"},{"location":"tutorials/distributedProcessing/","page":"Advanced distributed processing","title":"Advanced distributed processing","text":"Finally, for distributed computation of per-cluster mean, the clustering information needs to be distributed as well (Fortunately, that is easy, because the distributed mapToGigaSOM does exactly that).","category":"page"},{"location":"tutorials/distributedProcessing/","page":"Advanced distributed processing","title":"Advanced distributed processing","text":"First, compute the clustering:","category":"page"},{"location":"tutorials/distributedProcessing/","page":"Advanced distributed processing","title":"Advanced distributed processing","text":"mapping = mapToGigaSOM(som, di)\ndistributed_transform(mapping, m -> metaClusters[m])","category":"page"},{"location":"tutorials/distributedProcessing/","page":"Advanced distributed processing","title":"Advanced distributed processing","text":"Now, the distributed computation is run on 2 scattered datasets. We employ a helper function mapbuckets which provides bucket-wise execution of a function, in a way very similar to mapslices. (In the example, we actually use catmapbuckets that concatenates the result into a nice array.) The following code produces a matrix of tuples (sum, count), for separate clusters (in rows) and data columns (in columns):","category":"page"},{"location":"tutorials/distributedProcessing/","page":"Advanced distributed processing","title":"Advanced distributed processing","text":"sumscounts = distributed_mapreduce([di, mapping],\n    (d, mapping) -> catmapbuckets(\n        (_,clData) -> (sum(clData), length(clData)),\n\td, 10, mapping),\n    (a,b) -> (((as,al),(bs,bl)) -> ((as+bs), (al+bl))).(a,b))","category":"page"},{"location":"tutorials/distributedProcessing/","page":"Advanced distributed processing","title":"Advanced distributed processing","text":"10×4 Array{Tuple{Float64,Int64},2}:\n (5949.71, 1228)  (-21.9789, 1228)  (12231.3, 1228)  (12303.1, 1228)\n (6379.98, 1246)  (12464.3, 1246)   (12427.9, 1246)  (12479.8, 1246)\n (6513.41, 1294)  (12968.8, 1294)   (12960.7, 1294)  (-28.1922, 1294)\n (6312.37, 1236)  (-26.7392, 1236)  (6.74384, 1236)  (12401.7, 1236)\n (6395.73, 1285)  (12867.7, 1285)   (-52.653, 1285)  (-26.9795, 1285)\n (6229.72, 622)   (10.7578, 622)    (6200.1, 622)    (0.882128, 622)\n (6141.97, 612)   (6078.56, 612)    (45.9878, 612)   (6079.3, 612)\n (51.3709, 616)   (23.4306, 616)    (6117.53, 616)   (1.15342, 616)\n (6177.16, 1207)  (-50.4624, 1207)  (48.8023, 1207)  (-5.549, 1207)\n (8.56597, 654)   (6536.1, 654)     (-29.2208, 654)  (6539.94, 654)","category":"page"},{"location":"tutorials/distributedProcessing/","page":"Advanced distributed processing","title":"Advanced distributed processing","text":"With a bit of Julia, this can be aggregated to actual per-cluster means:","category":"page"},{"location":"tutorials/distributedProcessing/","page":"Advanced distributed processing","title":"Advanced distributed processing","text":"clusterMeans = [ sum/count for (sum,count) in sumcounts ]","category":"page"},{"location":"tutorials/distributedProcessing/","page":"Advanced distributed processing","title":"Advanced distributed processing","text":"10×4 Array{Float64,2}:\n  4.84504    -0.0178982   9.96031     10.0188\n  5.12037    10.0034      9.97428     10.0159\n  5.03354    10.0223     10.016       -0.0217869\n  5.10709    -0.0216336   0.00545618  10.0337\n  4.97722    10.0138     -0.0409751   -0.0209958\n 10.0156      0.0172955   9.968        0.00141821\n 10.0359      9.93229     0.0751434    9.9335\n  0.0833944   0.0380366   9.93105      0.00187243\n  5.11778    -0.0418081   0.0404327   -0.00459735\n  0.0130978   9.99403    -0.0446802    9.99991","category":"page"},{"location":"tutorials/distributedProcessing/","page":"Advanced distributed processing","title":"Advanced distributed processing","text":"Since we used the data from the hypercube dataset from the beginning of the tutorial, you should be able to recognize several clusters that perfectly match the hypercube vertices (although not all, because k=10 is not enough to capture all of the actual 16 existing clusters)","category":"page"},{"location":"tutorials/distributedProcessing/","page":"Advanced distributed processing","title":"Advanced distributed processing","text":"Finally, we can remove the temporary data from workers to create free memory for other analyses:","category":"page"},{"location":"tutorials/distributedProcessing/","page":"Advanced distributed processing","title":"Advanced distributed processing","text":"undistribute(mapping)","category":"page"},{"location":"tutorials/distributedProcessing/#Convenience-statistical-functions","page":"Advanced distributed processing","title":"Convenience statistical functions","text":"","category":"section"},{"location":"tutorials/distributedProcessing/","page":"Advanced distributed processing","title":"Advanced distributed processing","text":"Notably, several of the most used statistical functions are available in GigaSOM.jl in a form that can cope with distributed data.","category":"page"},{"location":"tutorials/distributedProcessing/","page":"Advanced distributed processing","title":"Advanced distributed processing","text":"For example, you can run a distributed median computation as such:","category":"page"},{"location":"tutorials/distributedProcessing/","page":"Advanced distributed processing","title":"Advanced distributed processing","text":"dmedian(di, [1,2,3,4])","category":"page"},{"location":"tutorials/distributedProcessing/","page":"Advanced distributed processing","title":"Advanced distributed processing","text":"In the hypercube dataset, the medians are slightly off-center because there is a lot of empty space between the clusters:","category":"page"},{"location":"tutorials/distributedProcessing/","page":"Advanced distributed processing","title":"Advanced distributed processing","text":"3-element Array{Float64,1}:\n 6.947097488861494\n 7.934405685940568\n 7.069149844215707\n 2.558892109203585","category":"page"},{"location":"tutorials/distributedProcessing/","page":"Advanced distributed processing","title":"Advanced distributed processing","text":"dstat function has a bucketed variant that can split the statistics among different clusters. This computes the per-cluster standard deviations of the dataset:","category":"page"},{"location":"tutorials/distributedProcessing/","page":"Advanced distributed processing","title":"Advanced distributed processing","text":"dstat_buckets(di, 10, mapping, [1,2,3,4])[2]","category":"page"},{"location":"tutorials/distributedProcessing/","page":"Advanced distributed processing","title":"Advanced distributed processing","text":"In the result, we can count 4 \"nice\" clusters, and 6 clusters that span 2 of the original clusters, totally giving 16. (Hypercube validation succeeded!)","category":"page"},{"location":"tutorials/distributedProcessing/","page":"Advanced distributed processing","title":"Advanced distributed processing","text":"10×4 Array{Float64,2}:\n 5.09089   0.997824  1.01815   0.980758\n 5.13971   1.02019   0.977637  1.00124\n 5.13209   0.974332  1.00058   0.99874\n 5.11529   0.998166  1.01825   1.01885\n 5.10542   1.01686   0.975993  0.991992\n 0.991075  0.993312  1.00667   1.05048\n 0.996443  1.02699   0.938742  0.98831\n 0.946917  0.989543  1.0056    0.999609\n 5.09963   1.00131   0.978803  0.984435\n 1.00892   0.998226  1.05538   0.994829","category":"page"},{"location":"tutorials/distributedProcessing/","page":"Advanced distributed processing","title":"Advanced distributed processing","text":"A similar bucketed version is available for computation of medians:","category":"page"},{"location":"tutorials/distributedProcessing/","page":"Advanced distributed processing","title":"Advanced distributed processing","text":"dmedian_buckets(di, 10, mapping, [1,2,3,4])","category":"page"},{"location":"tutorials/distributedProcessing/","page":"Advanced distributed processing","title":"Advanced distributed processing","text":"Note that the cluster medians are similar to means, except for the cases when the cluster is formed by 2 actual data aggregations (e.g. on the second row), where medians dodge the empty space in the middle of the data:","category":"page"},{"location":"tutorials/distributedProcessing/","page":"Advanced distributed processing","title":"Advanced distributed processing","text":"10×4 Array{Float64,2}:\n  1.97831    -0.0120118    9.98967    10.0161\n  7.99438    10.0263       9.9988     10.0033\n  3.27907     9.98728     10.0254      0.00444198\n  7.91739    -0.0623953   -0.0240277  10.0374\n  2.445      10.0101      -0.0471141  -0.0253346\n 10.0121      0.00935064   9.94992     0.0459787\n 10.0512      9.93359      0.0923141   9.91175\n  0.0675462  -0.0142712    9.93406     0.0343599\n  8.09972    -0.0217352    0.0575258  -0.010485\n -0.0183372  10.0392      -0.115253   10.0101","category":"page"},{"location":"background/#Background","page":"Background","title":"Background","text":"","category":"section"},{"location":"background/#Introduction","page":"Background","title":"Introduction","text":"","category":"section"},{"location":"background/","page":"Background","title":"Background","text":"Flow cytometry clustering for several hundred million cells has long been hampered by software limitations. Julia allows us to go beyond these limits. Through the high-performance GigaSOM.jl package, we gear up for huge-scale flow cytometry analysis, softening these limitations with an innovative approach on the existing algorithm, by implementing parallel computing with stable and robust mathematical models that would run with an HPC cluster, allowing us to run cytometry datasets as big as 500 million cells.","category":"page"},{"location":"background/#Multidimensional-Data","page":"Background","title":"Multidimensional Data","text":"","category":"section"},{"location":"background/","page":"Background","title":"Background","text":"Multidimensional data is a term used when combining four, five (or more) characteristics simultaneously to create a data space where each measured cell parameter becomes a dimension in the multidimensional space.\nThe data generated by Flow Cytometry, Mass Cytometry or RNA-seq are good examples of multidimensional datasets, as they combine all information from all characteristics to create a multidimensional data space that preserves the integrity of the relationships between each of the parameters.[1]","category":"page"},{"location":"background/#Flow-and-Mass-Cytometry","page":"Background","title":"Flow and Mass Cytometry","text":"","category":"section"},{"location":"background/","page":"Background","title":"Background","text":"The use of flow cytometry has grown substantially in the past decade, mainly due to the development of smaller, user-friendly and less expensive instruments, but also to the increase of clinical applications, like cell counting, cell sorting, detection of biomarkers or protein engineering. Flow cytometry is an immunophenotyping technique used to identify and quantify the cells of the immune system by analysing their physical and chemical characteristics in a fluid. These cells are stained with specific, fluorescently labelled antibodies and then analysed with a flow cytometer, where the fluorescence intensity is measured using lasers and photodetectors. [2] More recently, a variation of flow cytometry called mass cytometry (CyTOF) was introduced, in which antibodies are labelled with heavy metal ion tags rather than fluorochromes, breaking the limit of multiplexing capability of FACS (fluorescence-activated cell sorting) and allowing the simultaneous quantification of 40+ protein parameters within each single cell. The ability of flow cytometry and mass cytometry to analyse individual cells at high-throughput scales makes them ideal for multi-parameter cell analysis and high-speed sorting. [3]","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"(Image: MassCytometry)\nGeneral workflow of cell analysis in a mass cytometer. The cells are introduced into the nebulizer via a narrow capillary. As the cells exit from the nebulizer they are converted to a fine spray of droplets, which are then carried into the plasma where they are completely atomized and ionized. The resulting ion cloud is filtered and selected for positive ions of mass range 80–200 and measured in a TOF chamber. The data are then converted to .fcs format and analyzed using traditional flow cytometry software. [4]","category":"page"},{"location":"background/#Self-organising-maps-(SOMs)","page":"Background","title":"Self-organising maps (SOMs)","text":"","category":"section"},{"location":"background/","page":"Background","title":"Background","text":"Self-organising maps (also referred to as SOMs or Kohonen maps) are artificial neural networks introduced by Teuvo Kohonen in the 1980s. Despite of their age, SOMs are still widely used as an easy and robust unsupervised learning technique for analysis and visualisation of high-dimensional data. The SOM algorithm maps high-dimensional vectors into a lower-dimensional grid. Most often the target grid is two-dimensional, resulting into  intuitively interpretable maps. After initializing a SOM grid of size n*n, each node is initialized with a random sample (row) from the dataset (training data). For each input vector (row) in the training data the distance to each node in the grid is calculated, using Chebyshev distance or Euclidean distance equations, where the closest node is called BMU (best matching unit). The row is subsequently assigned to the BMU making it move closer to the input data, influenced by the learning rate and neighborhood Gaussian function, whilst the neighborhood nodes are also adjusted closer to the BMU. This training step is repeated for each  row in the complete dataset. After each iteration (epoch) the radius of the neighborhood function is reduced. After n epochs, clusters of nodes should have formed and as a final step, consensus cluster is used to reduce the data (SOM nodes) into m clusters. [5]","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"(Image: SOMs)\nExample of a self-organizing network with five cluster units, Yi, and seven input units, Xi.  The five cluster units are arranged in a linear array. [6]","category":"page"},{"location":"background/#Implementation","page":"Background","title":"Implementation","text":"","category":"section"},{"location":"background/","page":"Background","title":"Background","text":"The high-performance GigaSOM.jl package enables the analysis and clustering of huge-scale flow cytometry data because it is HPC-ready and written in Julia, prepared to handle very large datasets. Also, the GigaSOM.jl package, provides training and visualization functions for Kohonen's self-organizing maps for Julia. Training functions are implemented in pure Julia, without depending on additional binary libraries. The SOM algorithm maps high-dimensional vectors into a lower-dimensional grid. Most often, the target grid is two-dimensional, resulting into intuitively interpretable maps. The general idea is to receive huge-scale .fcs data files as an input, load and transform them accordingly to enable the analysis and clustering of these data and automatically determine the required number of cell populations, and their sensitivity and specificity using parallel computing. In order to achieve this, GigaSOM.jl implementes a batch SOM algorithm, in which the weight vectors are only updated at the end of each epoch, and the BMU is calculated using the weight vectors from the previous epoch. Since the weight updates are not recursive, the order of the inputs does not affect the final result. In addition, there is no learning rate coefficient, which reduces the data dependency. This means that, compared to the original one, the batch algorithm has faster convergence, requires less computing, and it is the only one suitable for parallel computing. Parallel computing is the last step of our package implementation and two possible approaches exist for this kind of computation: the Model Parallelism and the Data Parallelism. In the Model Parallelism approach, the algorithm sends the same data to all the processes, and then each process is responsible for estimating different parameters and exchange their estimates with each other to come up with the right estimate for all the parameters. In the Data Parallelism approach, the algorithm distributes the data between different processes, and then each process independently tries to estimate the same parameters and then exchange their estimates with each other to come up with the right estimate. On this project, we use the Data Parallelism approaches because our nodes grid is too small for the Model Parallelism approach.","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"(Image: parallel)\nData Parallelism vs Model Parallelism [7]","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: GigaSOM.jl)","category":"page"},{"location":"#GigaSOM.jl-Huge-scale,-high-performance-flow-cytometry-clustering-in-Julia","page":"Home","title":"GigaSOM.jl - Huge-scale, high-performance flow cytometry clustering in Julia","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"GigaSOM.jl allows painless analysis of huge-scale clinical studies, scaling down the software limitations that usually prevent work with large datasets. It can be viewed as a work-alike of FlowSOM, suitable for loading billions of cells and running the analyses in parallel on distributed computer clusters, to gain speed. Most importantly, GigaSOM.jl scales horizontally – data volume limitations and memory limitations can be solved just by adding more computers to the cluster. That makes it extremely easy to exploit HPC environments, which are becoming increasingly common in computational biology.","category":"page"},{"location":"","page":"Home","title":"Home","text":"<style type=\"text/css\">\n.evo {\n    width: 250px;\n    margin: 1em;\n    border-radius: 50%;\n    -webkit-border-radius: 50%;\n    -moz-border-radius: 50%;\n}\n</style>\n<div align=\"center\">\n    <img class=\"evo\" src=\"https://webdav-r3lab.uni.lu/public/GigaSOM/img/evolution.gif\">\n    <br/>\n    <b>Evolution of the GigaSOM.jl repository (2019-2020)</b>\n</div>","category":"page"},{"location":"#Features","page":"Home","title":"Features","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Horizontal scalability to literal giga-scale datasets (10^9 cells!)\nHPC-ready, support for e.g. Slurm\nStandard support for distributed loading, scaling and transforming the FCS3 files\nBatch-SOM based GigaSOM algorithm for clustering\nEmbedSOM for visualizations","category":"page"},{"location":"#Background","page":"Home","title":"Background","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"You can learn more about the background of GigaSOM.jl in these sections:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Pages = [\"background.md\"]","category":"page"},{"location":"#How-to-get-started?","page":"Home","title":"How to get started?","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"You can follow our extensive tutorials here:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Pages = [\"tutorials/basicUsage.md\",\n        \"tutorials/processingFCSData.md\",\n        \"tutorials/distributedProcessing.md\",\n        \"tutorials/whereToGoNext.md\"\n]","category":"page"},{"location":"#Functions","page":"Home","title":"Functions","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"A full reference to all functions is given here:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Pages = [\"functions.md\"]","category":"page"},{"location":"#How-to-contribute?","page":"Home","title":"How to contribute?","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you want to contribute, please read these guidelines first:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Pages = [\"howToContribute.md\"]","category":"page"}]
}
